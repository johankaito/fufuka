[2015-07-21 17:14:53,258] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,261] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,261] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,262] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,262] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,262] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,262] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,262] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 19 (state.change.logger)
[2015-07-21 17:14:53,277] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 20 from controller 3 epoch 108 for partition [t1,1] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 20 from controller 3 epoch 108 for partition [t1,5] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 20 from controller 3 epoch 108 for partition [t1,6] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 20 from controller 3 epoch 108 for partition [t1,0] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 20 from controller 3 epoch 108 for partition [t1,2] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 20 from controller 3 epoch 108 for partition [t1,4] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 20 from controller 3 epoch 108 for partition [t1,7] (state.change.logger)
[2015-07-21 17:14:53,282] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 20 from controller 3 epoch 108 for partition [t1,3] (state.change.logger)
[2015-07-21 17:14:53,283] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,1] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,283] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,5] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,283] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,6] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,283] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,0] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,283] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,2] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,284] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,4] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,284] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,7] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,284] WARN Broker 1 ignoring LeaderAndIsr request from controller 3 with correlation id 20 epoch 108 for partition [t1,3] since its associated leader epoch 0 is old. Current leader epoch is 0 (state.change.logger)
[2015-07-21 17:14:53,286] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,287] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:0,ControllerEpoch:107),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 20 (state.change.logger)
[2015-07-21 17:14:53,335] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 21 from controller 3 epoch 108 for partition [t1,1] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,3,2,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 21 from controller 3 epoch 108 for partition [t1,5] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 21 from controller 3 epoch 108 for partition [t1,6] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 21 from controller 3 epoch 108 for partition [t1,0] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,3,2,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 21 from controller 3 epoch 108 for partition [t1,2] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 21 from controller 3 epoch 108 for partition [t1,4] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 21 from controller 3 epoch 108 for partition [t1,7] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 21 from controller 3 epoch 108 for partition [t1,3] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:14:53,336] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 108 with correlation id 21 for partition [t1,5] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 108 with correlation id 21 for partition [t1,2] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:14:53,337] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:14:53,338] TRACE Broker 1 handling LeaderAndIsr request correlationId 21 from controller 3 epoch 108 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:14:53,338] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,4] since the new leader 3 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,339] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,1] since the new leader 3 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,339] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,6] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,339] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,3] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,339] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,0] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,339] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 21 from controller 3 epoch 108 for partition [t1,7] since the new leader 3 is the same as the old leader (state.change.logger)
[2015-07-21 17:14:53,369] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:14:53,369] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:14:53,369] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:14:53,369] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:14:53,369] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:14:53,370] TRACE Broker 1 completed LeaderAndIsr request correlationId 21 from controller 3 epoch 108 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,3,2,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,3,2,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:3,2,1,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,371] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,3,LeaderEpoch:1,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 21 (state.change.logger)
[2015-07-21 17:14:53,372] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:14:53,372] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,372] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:14:53,375] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:14:53,375] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,375] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:14:53,376] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:14:53,376] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,376] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:14:53,377] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:14:53,377] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,377] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:14:53,389] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:14:53,389] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,389] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:14:53,390] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:14:53,390] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,390] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:14:53,392] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:14:53,392] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,392] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:14:53,393] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:14:53,393] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,393] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:14:53,412] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:14:53,413] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,413] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:14:53,413] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:14:53,414] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,414] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:14:53,425] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:14:53,425] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,425] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:14:53,427] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:14:53,427] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,427] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:14:53,428] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:14:53,428] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,428] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:14:53,430] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:14:53,430] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,430] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:14:53,430] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:14:53,430] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,431] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:14:53,434] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:14:53,434] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,434] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:14:53,435] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:14:53,435] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,435] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:14:53,436] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:14:53,436] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,436] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:14:53,437] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:14:53,437] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,437] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:14:53,438] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:14:53,438] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,438] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:14:53,439] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:14:53,439] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,439] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:14:53,445] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:14:53,445] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,445] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:14:53,446] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:14:53,446] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,446] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:14:53,455] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:14:53,456] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,456] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:14:53,456] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:14:53,457] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,457] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:14:53,457] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:14:53,457] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,457] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:14:53,469] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:14:53,469] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,469] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:14:53,470] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:14:53,470] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,470] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,471] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:14:53,495] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:14:53,495] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,495] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:14:53,496] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:14:53,496] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:14:53,496] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:29:12,698] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,2,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 35 from controller 3 epoch 108 for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,698] TRACE Broker 1 handling LeaderAndIsr request correlationId 35 from controller 3 epoch 108 starting the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,699] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 108 with correlation id 35 for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,700] TRACE Broker 1 completed LeaderAndIsr request correlationId 35 from controller 3 epoch 108 for the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,702] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,2,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 35 (state.change.logger)
[2015-07-21 17:29:12,707] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,2,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 37 from controller 3 epoch 108 for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,707] TRACE Broker 1 handling LeaderAndIsr request correlationId 37 from controller 3 epoch 108 starting the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,707] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 108 with correlation id 37 for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,707] TRACE Broker 1 completed LeaderAndIsr request correlationId 37 from controller 3 epoch 108 for the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,709] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,2,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 37 (state.change.logger)
[2015-07-21 17:29:12,713] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 39 from controller 3 epoch 108 for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,713] TRACE Broker 1 handling LeaderAndIsr request correlationId 39 from controller 3 epoch 108 starting the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,714] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 39 from controller 3 epoch 108 for partition [t1,6] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:29:12,715] TRACE Broker 1 completed LeaderAndIsr request correlationId 39 from controller 3 epoch 108 for the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,716] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 39 (state.change.logger)
[2015-07-21 17:29:12,721] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 40 from controller 3 epoch 108 for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,721] TRACE Broker 1 handling LeaderAndIsr request correlationId 40 from controller 3 epoch 108 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,721] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 108 with correlation id 40 for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,723] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,1] as part of become-follower request with correlation id 40 from controller 3 epoch 108 (state.change.logger)
[2015-07-21 17:29:12,723] TRACE Broker 1 skipped the adding-fetcher step of the become-follower state change with correlation id 40 from controller 3 epoch 108 for partition [t1,1] since it is shutting down (state.change.logger)
[2015-07-21 17:29:12,724] TRACE Broker 1 completed LeaderAndIsr request correlationId 40 from controller 3 epoch 108 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,724] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 40 (state.change.logger)
[2015-07-21 17:29:12,727] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 42 from controller 3 epoch 108 for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,727] TRACE Broker 1 handling LeaderAndIsr request correlationId 42 from controller 3 epoch 108 starting the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,727] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 42 from controller 3 epoch 108 for partition [t1,0] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:29:12,728] TRACE Broker 1 completed LeaderAndIsr request correlationId 42 from controller 3 epoch 108 for the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,729] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 42 (state.change.logger)
[2015-07-21 17:29:12,734] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 43 from controller 3 epoch 108 for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,734] TRACE Broker 1 handling LeaderAndIsr request correlationId 43 from controller 3 epoch 108 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,735] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 108 with correlation id 43 for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,737] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,7] as part of become-follower request with correlation id 43 from controller 3 epoch 108 (state.change.logger)
[2015-07-21 17:29:12,737] TRACE Broker 1 skipped the adding-fetcher step of the become-follower state change with correlation id 43 from controller 3 epoch 108 for partition [t1,7] since it is shutting down (state.change.logger)
[2015-07-21 17:29:12,737] TRACE Broker 1 completed LeaderAndIsr request correlationId 43 from controller 3 epoch 108 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,739] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 43 (state.change.logger)
[2015-07-21 17:29:12,741] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 45 from controller 3 epoch 108 for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:12,742] TRACE Broker 1 handling LeaderAndIsr request correlationId 45 from controller 3 epoch 108 starting the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:12,742] INFO Broker 1 skipped the become-follower state change after marking its partition as follower with correlation id 45 from controller 3 epoch 108 for partition [t1,3] since the new leader 2 is the same as the old leader (state.change.logger)
[2015-07-21 17:29:12,743] TRACE Broker 1 completed LeaderAndIsr request correlationId 45 from controller 3 epoch 108 for the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:12,744] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 45 (state.change.logger)
[2015-07-21 17:29:12,748] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 46 from controller 3 epoch 108 for partition [t1,4] (state.change.logger)
[2015-07-21 17:29:12,748] TRACE Broker 1 handling LeaderAndIsr request correlationId 46 from controller 3 epoch 108 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:29:12,748] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 108 with correlation id 46 for partition [t1,4] (state.change.logger)
[2015-07-21 17:29:12,750] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,4] as part of become-follower request with correlation id 46 from controller 3 epoch 108 (state.change.logger)
[2015-07-21 17:29:12,750] TRACE Broker 1 skipped the adding-fetcher step of the become-follower state change with correlation id 46 from controller 3 epoch 108 for partition [t1,4] since it is shutting down (state.change.logger)
[2015-07-21 17:29:12,750] TRACE Broker 1 completed LeaderAndIsr request correlationId 46 from controller 3 epoch 108 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:29:12,764] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,1,LeaderEpoch:2,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 46 (state.change.logger)
[2015-07-21 17:29:12,766] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 47 from controller 3 epoch 108 for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,766] TRACE Broker 1 handling LeaderAndIsr request correlationId 47 from controller 3 epoch 108 starting the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,766] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 108 with correlation id 47 for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,768] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,5] as part of become-follower request with correlation id 47 from controller 3 epoch 108 (state.change.logger)
[2015-07-21 17:29:12,768] TRACE Broker 1 skipped the adding-fetcher step of the become-follower state change with correlation id 47 from controller 3 epoch 108 for partition [t1,5] since it is shutting down (state.change.logger)
[2015-07-21 17:29:12,768] TRACE Broker 1 completed LeaderAndIsr request correlationId 47 from controller 3 epoch 108 for the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:29:12,769] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 47 (state.change.logger)
[2015-07-21 17:29:12,771] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 48 from controller 3 epoch 108 for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,771] TRACE Broker 1 handling LeaderAndIsr request correlationId 48 from controller 3 epoch 108 starting the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,771] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 108 with correlation id 48 for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,772] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,2] as part of become-follower request with correlation id 48 from controller 3 epoch 108 (state.change.logger)
[2015-07-21 17:29:12,772] TRACE Broker 1 skipped the adding-fetcher step of the become-follower state change with correlation id 48 from controller 3 epoch 108 for partition [t1,2] since it is shutting down (state.change.logger)
[2015-07-21 17:29:12,772] TRACE Broker 1 completed LeaderAndIsr request correlationId 48 from controller 3 epoch 108 for the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:29:12,773] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 48 (state.change.logger)
[2015-07-21 17:29:12,774] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,774] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,776] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 50 (state.change.logger)
[2015-07-21 17:29:12,777] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,777] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,6] (state.change.logger)
[2015-07-21 17:29:12,778] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,778] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,780] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 52 (state.change.logger)
[2015-07-21 17:29:12,781] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,781] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,1] (state.change.logger)
[2015-07-21 17:29:12,783] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,783] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,785] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 54 (state.change.logger)
[2015-07-21 17:29:12,786] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,786] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,0] (state.change.logger)
[2015-07-21 17:29:12,787] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,787] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,789] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 56 (state.change.logger)
[2015-07-21 17:29:12,790] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,790] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,7] (state.change.logger)
[2015-07-21 17:29:12,790] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:12,790] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:12,793] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 108 with correlation id 58 (state.change.logger)
[2015-07-21 17:29:13,020] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:13,020] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,3] (state.change.logger)
[2015-07-21 17:29:13,022] TRACE Broker 1 handling stop replica (delete=false) for partition [t1,4] (state.change.logger)
[2015-07-21 17:29:13,022] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t1,4] (state.change.logger)
[2015-07-21 17:31:48,063] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@66d3c617, name=log4j:logger=state.change.logger (state.change.logger)
[2015-07-21 17:31:57,240] TRACE Controller 0 epoch 109 started leader election for partition [t1,5] (state.change.logger)
[2015-07-21 17:31:57,253] ERROR Controller 0 epoch 109 initiated state change for partition [t1,5] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,5] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(1, 2, 3)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,256] TRACE Controller 0 epoch 109 started leader election for partition [t1,2] (state.change.logger)
[2015-07-21 17:31:57,265] ERROR Controller 0 epoch 109 initiated state change for partition [t1,2] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,2] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(1, 3, 2)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,266] TRACE Controller 0 epoch 109 started leader election for partition [t1,0] (state.change.logger)
[2015-07-21 17:31:57,275] ERROR Controller 0 epoch 109 initiated state change for partition [t1,0] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,0] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(2, 1, 3)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,276] TRACE Controller 0 epoch 109 started leader election for partition [t1,1] (state.change.logger)
[2015-07-21 17:31:57,284] ERROR Controller 0 epoch 109 initiated state change for partition [t1,1] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,1] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(3, 2, 1)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,285] TRACE Controller 0 epoch 109 started leader election for partition [t1,6] (state.change.logger)
[2015-07-21 17:31:57,292] ERROR Controller 0 epoch 109 initiated state change for partition [t1,6] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,6] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(2, 1, 3)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,293] TRACE Controller 0 epoch 109 started leader election for partition [t1,3] (state.change.logger)
[2015-07-21 17:31:57,301] ERROR Controller 0 epoch 109 initiated state change for partition [t1,3] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,3] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(2, 3, 1)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,302] TRACE Controller 0 epoch 109 started leader election for partition [t1,7] (state.change.logger)
[2015-07-21 17:31:57,311] ERROR Controller 0 epoch 109 initiated state change for partition [t1,7] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,7] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(3, 2, 1)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,311] TRACE Controller 0 epoch 109 started leader election for partition [t1,4] (state.change.logger)
[2015-07-21 17:31:57,320] ERROR Controller 0 epoch 109 initiated state change for partition [t1,4] from OfflinePartition to OnlinePartition failed (state.change.logger)
kafka.common.NoReplicaOnlineException: No replica for partition [t1,4] is alive. Live brokers are: [Set(0)], Assigned replicas are: [List(3, 1, 2)]
	at kafka.controller.OfflinePartitionLeaderSelector.selectLeader(PartitionLeaderSelector.scala:75)
	at kafka.controller.PartitionStateMachine.electLeaderForPartition(PartitionStateMachine.scala:357)
	at kafka.controller.PartitionStateMachine.kafka$controller$PartitionStateMachine$$handleStateChange(PartitionStateMachine.scala:206)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:120)
	at kafka.controller.PartitionStateMachine$$anonfun$triggerOnlinePartitionStateChange$3.apply(PartitionStateMachine.scala:117)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:778)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:777)
	at kafka.controller.PartitionStateMachine.triggerOnlinePartitionStateChange(PartitionStateMachine.scala:117)
	at kafka.controller.PartitionStateMachine.startup(PartitionStateMachine.scala:70)
	at kafka.controller.KafkaController.onControllerFailover(KafkaController.scala:314)
	at kafka.controller.KafkaController$$anonfun$1.apply$mcV$sp(KafkaController.scala:161)
	at kafka.server.ZookeeperLeaderElector.elect(ZookeeperLeaderElector.scala:81)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply$mcZ$sp(ZookeeperLeaderElector.scala:139)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener$$anonfun$handleDataDeleted$1.apply(ZookeeperLeaderElector.scala:134)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.server.ZookeeperLeaderElector$LeaderChangeListener.handleDataDeleted(ZookeeperLeaderElector.scala:134)
	at org.I0Itec.zkclient.ZkClient$6.run(ZkClient.java:549)
	at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
[2015-07-21 17:31:57,358] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:31:57,358] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:31:57,358] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:31:57,358] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:31:57,359] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:31:57,359] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:31:57,359] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:31:57,360] TRACE Controller 0 epoch 109 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 2 to broker 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:31:57,397] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,397] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,397] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,397] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,398] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,398] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,398] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,398] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 0 epoch 109 with correlation id 2 (state.change.logger)
[2015-07-21 17:31:57,403] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,404] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,405] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,405] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,406] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,407] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,408] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,408] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,409] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,410] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,410] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,411] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,412] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,413] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,413] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,414] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,415] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,415] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,416] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,416] TRACE Controller 0 epoch 109 received response UpdateMetadataResponse(2,0) for a request sent to broker id:0,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:31:57,417] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,418] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t2,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,419] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,420] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t2,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,420] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t2,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,446] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,447] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,447] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,448] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,1] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,448] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,449] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,450] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,450] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,451] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,451] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,452] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,4] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,452] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,2] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,453] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,6] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,453] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,454] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,454] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,455] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,455] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,0] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,456] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,456] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,457] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,5] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,458] ERROR Controller 0 epoch 109 initiated state change of replica 2 for partition [t3,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,458] ERROR Controller 0 epoch 109 initiated state change of replica 1 for partition [t3,7] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:31:57,459] ERROR Controller 0 epoch 109 initiated state change of replica 3 for partition [t3,3] from ReplicaDeletionIneligible to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:103)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:103)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$markTopicForDeletionRetry(TopicDeletionManager.scala:270)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:416)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:00,322] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@66d3c617, name=log4j:logger=state.change.logger (state.change.logger)
[2015-07-21 17:33:09,281] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,3] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,283] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,3] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,285] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,5] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,287] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,0] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,289] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,4] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,291] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,7] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,293] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,6] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,295] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,7] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,297] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,1] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,300] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,5] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,302] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,1] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,303] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,4] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,305] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,2] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,306] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,0] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,307] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,2] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,309] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,2] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,311] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,6] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,312] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,1] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,314] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,4] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,316] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,5] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,317] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,0] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,319] TRACE Controller 3 epoch 110 changed state of replica 3 for partition [t1,7] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,321] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [t1,6] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,322] TRACE Controller 3 epoch 110 changed state of replica 2 for partition [t1,3] from OnlineReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,333] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,334] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,334] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,334] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,334] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,335] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,335] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,335] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,335] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,335] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,336] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,337] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,337] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,342] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,343] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 2 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,344] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,344] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,344] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,364] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,364] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 0 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,364] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,364] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,364] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 0 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,365] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 0 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,365] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,365] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 0 from controller 3 epoch 110 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,345] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,365] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 0 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 0 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 0 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,376] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 0 from controller 3 epoch 110 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,381] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,382] TRACE Broker 1 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,385] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,385] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,385] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,386] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,386] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,386] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,386] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,386] TRACE Broker 3 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,399] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,399] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 0 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,400] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,400] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) correlation id 0 from controller 3 epoch 110 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,400] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 0 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,400] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 0 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,400] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 0 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,401] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) correlation id 0 from controller 3 epoch 110 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,430] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,430] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,431] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,440] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 1 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Broker 2 handling LeaderAndIsr request correlationId 0 from controller 3 epoch 110 starting the become-leader transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,441] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,442] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,442] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 0 to broker 3 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,457] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,457] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,457] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,457] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,457] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,458] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,473] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,473] TRACE Broker 2 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,482] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,482] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 2 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,483] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 1 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,484] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108) with correlationId 3 to broker 3 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,1] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,4] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,485] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,0] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,486] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,7] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,486] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,6] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,486] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,3] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,486] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,2] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,486] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,5] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,530] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,544] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,548] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,549] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,550] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,551] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,551] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,552] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,553] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,553] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,554] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,555] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,555] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,556] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,556] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,557] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,559] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,560] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,560] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,561] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,562] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,1] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,562] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,563] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,564] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,564] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t2,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,565] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t2,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,562] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,4] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,567] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,0] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,567] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,7] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,567] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,6] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,568] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,3] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,568] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,2] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,568] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,5] as part of become-follower request with correlation id 0 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:09,576] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=3,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,576] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=1,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,577] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=5,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,577] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=6,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,578] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=4,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,578] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=2,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,579] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=3,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,580] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=4,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,580] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=2,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,591] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,591] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,592] TRACE Broker 2 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-leader transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,623] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,624] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,625] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,625] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,625] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,625] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,625] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,628] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,581] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=6,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,616] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(0,Map((t1,5) -> 0, (t1,2) -> 0, (t1,0) -> 0, (t1,1) -> 0, (t1,6) -> 0, (t1,3) -> 0, (t1,7) -> 0, (t1,4) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,632] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,632] TRACE Broker 1 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,638] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=3,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,639] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=0,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,639] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=5,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,640] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=6,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,681] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,682] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,682] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,682] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,682] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,682] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,683] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,683] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,666] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=1,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,685] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(0,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,688] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,688] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,688] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,689] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,689] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,689] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,689] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,689] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,690] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(3,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,698] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,698] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,699] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,692] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=2,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,709] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(0,Map((t1,5) -> 0, (t1,2) -> 0, (t1,0) -> 0, (t1,1) -> 0, (t1,6) -> 0, (t1,3) -> 0, (t1,7) -> 0, (t1,4) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,704] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,2] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,712] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=7,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,713] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,713] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,713] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,713] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=1,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,714] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,6] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,715] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,715] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,715] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,714] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t2,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=0,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,716] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,3] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,718] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,718] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,718] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,720] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,720] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,720] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,720] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,720] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,721] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,721] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,721] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,717] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=4,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,726] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=7,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,727] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=0,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,727] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t2,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=7,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,728] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t2,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t2,Partition=5,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,725] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,722] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(0,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,719] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,1] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,741] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,741] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,741] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,741] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,741] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,742] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,742] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,742] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,744] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,761] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,761] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 0 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,6] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,3] (state.change.logger)
[2015-07-21 17:33:09,762] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:33:09,763] TRACE Broker 3 completed LeaderAndIsr request correlationId 0 from controller 3 epoch 110 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:09,749] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,744] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(3,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,743] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,4] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,766] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,770] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,771] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,771] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,769] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,772] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,772] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,772] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,773] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,773] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,771] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,7] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,774] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,777] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,777] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,777] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,774] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,781] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(0,Map((t1,5) -> 0, (t1,2) -> 0, (t1,0) -> 0, (t1,1) -> 0, (t1,6) -> 0, (t1,3) -> 0, (t1,7) -> 0, (t1,4) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,785] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,785] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,786] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,778] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,0] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,775] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,1] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,792] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,792] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,793] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,793] TRACE Broker 2 handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,793] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t2,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,794] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,791] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,795] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,794] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,5] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,793] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,6] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,797] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,797] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,797] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,796] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,797] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,3] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,799] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,799] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,799] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,800] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,4] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,800] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,801] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,801] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,801] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,0] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,802] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,2] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,2] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,802] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,802] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,803] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,803] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,3] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,3] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,803] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,5] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,804] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,804] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,805] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,804] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,805] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,2] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,813] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,4] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,4] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,814] TRACE Broker 1 handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,814] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t2,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,814] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,814] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,7] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,7] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,815] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,7] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,815] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,816] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,816] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,1] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,1] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,817] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,6] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 2 for partition [t3,6] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,817] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,818] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,818] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,818] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,818] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,818] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,819] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,819] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 0 (state.change.logger)
[2015-07-21 17:33:09,819] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(0,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,821] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,821] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,821] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,6] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,1,3) for partition [t1,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:2,ISR:2,LeaderEpoch:3,ControllerEpoch:108),ReplicationFactor:3),AllReplicas:2,3,1) for partition [t1,3] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 3 (state.change.logger)
[2015-07-21 17:33:09,822] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(3,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,827] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,0] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 1 for partition [t3,0] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,827] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,828] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,5] from OnlineReplica to OfflineReplica failed (state.change.logger)
kafka.common.StateChangeFailedException: Failed to change state of replica 3 for partition [t3,5] since the leader and isr path in zookeeper is empty
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:269)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:335)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,828] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,829] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,3] (state.change.logger)
[2015-07-21 17:33:09,830] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,830] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,830] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,831] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,831] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,831] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,831] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,4] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,831] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,1] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,832] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,832] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,832] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,833] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,3] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,833] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,833] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,833] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,3] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,834] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,5] (state.change.logger)
[2015-07-21 17:33:09,835] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,5] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,835] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,5] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,837] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,837] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,837] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,835] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=1,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,838] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,2] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,838] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,2] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,836] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,838] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,839] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,4] (state.change.logger)
[2015-07-21 17:33:09,839] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,4] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,839] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,839] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,839] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,840] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,840] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,840] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,2] (state.change.logger)
[2015-07-21 17:33:09,840] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,7] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,2] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,841] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,6] (state.change.logger)
[2015-07-21 17:33:09,842] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,6] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,842] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,6] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,842] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,842] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,842] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,843] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,843] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,843] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,843] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,1] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,7] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Broker 1 handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Broker 1 ignoring stop replica (delete=false) for partition [t3,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Broker 1 finished handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,844] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,845] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,845] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,0] -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:09,845] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,0] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,845] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=3,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,846] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,846] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=4,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,846] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,846] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,846] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=1,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,847] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=2,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,848] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=2,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,848] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=7,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,849] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=3,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,850] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=5,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,850] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=0,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,847] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,5] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,851] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=6,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,852] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=7,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,852] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,852] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,852] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,852] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=5,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,853] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,2] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=2,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,854] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,3] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=3,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,854] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=4,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,847] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,855] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,855] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,4] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=4,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,856] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,7] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=7,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,856] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=6,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,857] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=0,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,857] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,1] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=1,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,858] ERROR Controller 3 epoch 110 initiated state change of replica 2 for partition [t3,6] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=6,Replica=2] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,853] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,3] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,858] ERROR Controller 3 epoch 110 initiated state change of replica 1 for partition [t3,0] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=0,Replica=1] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,855] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,7] (state.change.logger)
[2015-07-21 17:33:09,858] ERROR Controller 3 epoch 110 initiated state change of replica 3 for partition [t3,5] from OnlineReplica to ReplicaDeletionStarted failed (state.change.logger)
java.lang.AssertionError: assertion failed: Replica [Topic=t3,Partition=5,Replica=3] should be in the OfflineReplica states before moving to ReplicaDeletionStarted state. Instead it is in OnlineReplica state
	at scala.Predef$.assert(Predef.scala:165)
	at kafka.controller.ReplicaStateMachine.assertValidPreviousStates(ReplicaStateMachine.scala:309)
	at kafka.controller.ReplicaStateMachine.handleStateChange(ReplicaStateMachine.scala:190)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at kafka.controller.ReplicaStateMachine$$anonfun$handleStateChanges$2.apply(ReplicaStateMachine.scala:114)
	at scala.collection.immutable.HashSet$HashSet1.foreach(HashSet.scala:322)
	at scala.collection.immutable.HashSet$HashTrieSet.foreach(HashSet.scala:978)
	at kafka.controller.ReplicaStateMachine.handleStateChanges(ReplicaStateMachine.scala:114)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:337)
	at kafka.controller.TopicDeletionManager$$anonfun$startReplicaDeletion$2.apply(TopicDeletionManager.scala:327)
	at scala.collection.immutable.Map$Map1.foreach(Map.scala:116)
	at kafka.controller.TopicDeletionManager.startReplicaDeletion(TopicDeletionManager.scala:327)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onPartitionDeletion(TopicDeletionManager.scala:360)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:306)
	at kafka.controller.TopicDeletionManager$$anonfun$kafka$controller$TopicDeletionManager$$onTopicDeletion$2.apply(TopicDeletionManager.scala:305)
	at scala.collection.immutable.Set$Set1.foreach(Set.scala:79)
	at kafka.controller.TopicDeletionManager.kafka$controller$TopicDeletionManager$$onTopicDeletion(TopicDeletionManager.scala:305)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:424)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1$$anonfun$apply$mcV$sp$4.apply(TopicDeletionManager.scala:396)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:111)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply$mcV$sp(TopicDeletionManager.scala:396)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread$$anonfun$doWork$1.apply(TopicDeletionManager.scala:390)
	at kafka.utils.Utils$.inLock(Utils.scala:535)
	at kafka.controller.TopicDeletionManager$DeleteTopicsThread.doWork(TopicDeletionManager.scala:390)
	at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:60)
[2015-07-21 17:33:09,859] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,7] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,859] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,859] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,860] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,860] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,860] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,860] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,1] (state.change.logger)
[2015-07-21 17:33:09,860] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,1] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,861] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,4] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,861] TRACE Broker 3 handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,861] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t2,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,861] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t2,0] (state.change.logger)
[2015-07-21 17:33:09,862] TRACE Controller 3 epoch 110 received response StopReplicaResponse(5,Map([t2,0] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,863] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,863] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,3] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,863] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,3] (state.change.logger)
[2015-07-21 17:33:09,863] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,3] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,864] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,864] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,1] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,864] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,1] (state.change.logger)
[2015-07-21 17:33:09,864] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,1] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,865] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,865] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,2] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,865] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,2] (state.change.logger)
[2015-07-21 17:33:09,866] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,2] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,866] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,866] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,866] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,6] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,7] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 2 handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,7] (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 2 ignoring stop replica (delete=false) for partition [t3,6] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,867] TRACE Broker 2 finished handling stop replica (delete=false) for partition [t3,6] (state.change.logger)
[2015-07-21 17:33:09,868] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,7] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,868] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,6] -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:09,868] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,869] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,4] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,869] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,4] (state.change.logger)
[2015-07-21 17:33:09,869] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,4] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,870] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,870] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,0] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,870] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,0] (state.change.logger)
[2015-07-21 17:33:09,871] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,0] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:09,871] TRACE Broker 3 handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,871] TRACE Broker 3 ignoring stop replica (delete=false) for partition [t3,5] as replica doesn't exist on broker (state.change.logger)
[2015-07-21 17:33:09,871] TRACE Broker 3 finished handling stop replica (delete=false) for partition [t3,5] (state.change.logger)
[2015-07-21 17:33:09,872] TRACE Controller 3 epoch 110 received response StopReplicaResponse(8,Map([t3,5] -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,499] TRACE Controller 3 epoch 110 started leader election for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,506] TRACE Controller 3 epoch 110 elected leader 1 for Offline partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,508] TRACE Controller 3 epoch 110 changed partition [t1,5] from OnlinePartition to OnlinePartition with leader 1 (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 2 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 1 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 3 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 2 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 1 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,509] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 10 to broker 3 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,510] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 10 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,511] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 10 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,511] TRACE Broker 3 handling LeaderAndIsr request correlationId 10 from controller 3 epoch 110 starting the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,512] TRACE Broker 2 handling LeaderAndIsr request correlationId 10 from controller 3 epoch 110 starting the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,512] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) correlation id 10 from controller 3 epoch 110 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,512] TRACE Controller 3 epoch 110 started leader election for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,513] TRACE Broker 1 handling LeaderAndIsr request correlationId 10 from controller 3 epoch 110 starting the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,516] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 10 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,518] TRACE Broker 2 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 10 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,519] TRACE Controller 3 epoch 110 elected leader 1 for Offline partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,520] TRACE Controller 3 epoch 110 changed partition [t1,2] from OnlinePartition to OnlinePartition with leader 1 (state.change.logger)
[2015-07-21 17:33:14,521] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 2 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,521] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 1 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,521] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 3 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,521] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 2 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,521] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 1 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,522] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 11 to broker 3 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,522] TRACE Controller 3 epoch 110 started leader election for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,524] TRACE Broker 1 completed LeaderAndIsr request correlationId 10 from controller 3 epoch 110 for the become-leader transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,525] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(10,Map((t1,5) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,526] TRACE Broker 2 truncated logs and checkpointed recovery boundaries for partition [t1,5] as part of become-follower request with correlation id 10 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,527] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 10 (state.change.logger)
[2015-07-21 17:33:14,528] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(10,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,529] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 11 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,529] TRACE Broker 1 handling LeaderAndIsr request correlationId 11 from controller 3 epoch 110 starting the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,530] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 11 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,530] TRACE Broker 1 completed LeaderAndIsr request correlationId 11 from controller 3 epoch 110 for the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,532] TRACE Controller 3 epoch 110 elected leader 3 for Offline partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,532] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(11,Map((t1,2) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 changed partition [t1,1] from OnlinePartition to OnlinePartition with leader 3 (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 11 (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 2 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 1 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 3 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 2 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 1 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(11,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,534] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 12 to broker 3 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,535] TRACE Controller 3 epoch 110 started leader election for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,535] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 12 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,536] TRACE Broker 1 handling LeaderAndIsr request correlationId 12 from controller 3 epoch 110 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,536] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 12 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,539] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,1] as part of become-follower request with correlation id 12 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,542] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 12 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,542] TRACE Broker 1 completed LeaderAndIsr request correlationId 12 from controller 3 epoch 110 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,543] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(12,Map((t1,1) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,546] TRACE Controller 3 epoch 110 elected leader 3 for Offline partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,548] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 12 (state.change.logger)
[2015-07-21 17:33:14,548] TRACE Controller 3 epoch 110 changed partition [t1,7] from OnlinePartition to OnlinePartition with leader 3 (state.change.logger)
[2015-07-21 17:33:14,548] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 2 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,548] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 1 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,549] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 3 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,549] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 2 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,549] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 1 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,549] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 13 to broker 3 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,550] TRACE Controller 3 epoch 110 started leader election for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,551] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(12,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,556] TRACE Broker 2 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 10 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,557] TRACE Broker 2 completed LeaderAndIsr request correlationId 10 from controller 3 epoch 110 for the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,559] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 13 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,559] TRACE Broker 1 handling LeaderAndIsr request correlationId 13 from controller 3 epoch 110 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,560] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 13 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,560] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 10 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,563] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,7] as part of become-follower request with correlation id 13 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,563] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 13 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,563] TRACE Broker 1 completed LeaderAndIsr request correlationId 13 from controller 3 epoch 110 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,583] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,5] as part of become-follower request with correlation id 10 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,584] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(13,Map((t1,7) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,585] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(10,Map((t1,5) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,588] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 10 (state.change.logger)
[2015-07-21 17:33:14,588] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 13 (state.change.logger)
[2015-07-21 17:33:14,590] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(10,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,590] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(13,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,591] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 11 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,592] TRACE Broker 2 handling LeaderAndIsr request correlationId 11 from controller 3 epoch 110 starting the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,593] TRACE Broker 2 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 11 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,596] TRACE Broker 2 truncated logs and checkpointed recovery boundaries for partition [t1,2] as part of become-follower request with correlation id 11 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,596] TRACE Broker 2 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 11 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,596] TRACE Broker 2 completed LeaderAndIsr request correlationId 11 from controller 3 epoch 110 for the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,600] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(11,Map((t1,2) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,601] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 11 (state.change.logger)
[2015-07-21 17:33:14,602] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(11,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,602] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 12 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,603] TRACE Broker 2 handling LeaderAndIsr request correlationId 12 from controller 3 epoch 110 starting the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,603] TRACE Broker 2 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 12 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,605] TRACE Broker 2 truncated logs and checkpointed recovery boundaries for partition [t1,1] as part of become-follower request with correlation id 12 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,608] TRACE Broker 2 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 12 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,608] TRACE Broker 2 completed LeaderAndIsr request correlationId 12 from controller 3 epoch 110 for the become-follower transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,609] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(12,Map((t1,1) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,610] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 12 (state.change.logger)
[2015-07-21 17:33:14,617] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(12,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,618] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 13 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,618] TRACE Broker 2 handling LeaderAndIsr request correlationId 13 from controller 3 epoch 110 starting the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,618] TRACE Broker 2 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 13 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,621] TRACE Broker 2 truncated logs and checkpointed recovery boundaries for partition [t1,7] as part of become-follower request with correlation id 13 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,622] TRACE Broker 2 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 13 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,622] TRACE Broker 2 completed LeaderAndIsr request correlationId 13 from controller 3 epoch 110 for the become-follower transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,623] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(13,Map((t1,7) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,624] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 13 (state.change.logger)
[2015-07-21 17:33:14,624] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 10 for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,624] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(13,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,624] TRACE Broker 3 completed LeaderAndIsr request correlationId 10 from controller 3 epoch 110 for the become-follower transition for partition [t1,5] (state.change.logger)
[2015-07-21 17:33:14,625] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(10,Map((t1,5) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,626] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,2,3) for partition [t1,5] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 10 (state.change.logger)
[2015-07-21 17:33:14,627] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(10,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,628] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) correlation id 11 from controller 3 epoch 110 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,628] TRACE Broker 3 handling LeaderAndIsr request correlationId 11 from controller 3 epoch 110 starting the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,628] TRACE Broker 3 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 11 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,631] TRACE Broker 3 truncated logs and checkpointed recovery boundaries for partition [t1,2] as part of become-follower request with correlation id 11 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,631] TRACE Broker 3 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 11 for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,631] TRACE Broker 3 completed LeaderAndIsr request correlationId 11 from controller 3 epoch 110 for the become-follower transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:33:14,632] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(11,Map((t1,2) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,632] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:1,3,2) for partition [t1,2] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 11 (state.change.logger)
[2015-07-21 17:33:14,633] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(11,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,634] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 12 from controller 3 epoch 110 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,635] TRACE Broker 3 handling LeaderAndIsr request correlationId 12 from controller 3 epoch 110 starting the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,637] TRACE Broker 3 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 12 for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,645] TRACE Broker 3 completed LeaderAndIsr request correlationId 12 from controller 3 epoch 110 for the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:33:14,668] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(12,Map((t1,1) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,669] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,1] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 12 (state.change.logger)
[2015-07-21 17:33:14,669] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(12,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,670] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) correlation id 13 from controller 3 epoch 110 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,670] TRACE Broker 3 handling LeaderAndIsr request correlationId 13 from controller 3 epoch 110 starting the become-leader transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,670] TRACE Broker 3 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 13 for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,671] TRACE Broker 3 completed LeaderAndIsr request correlationId 13 from controller 3 epoch 110 for the become-leader transition for partition [t1,7] (state.change.logger)
[2015-07-21 17:33:14,671] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(13,Map((t1,7) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,672] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,2,1) for partition [t1,7] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 13 (state.change.logger)
[2015-07-21 17:33:14,672] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(13,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,705] TRACE Controller 3 epoch 110 elected leader 3 for Offline partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,707] TRACE Controller 3 epoch 110 changed partition [t1,4] from OnlinePartition to OnlinePartition with leader 3 (state.change.logger)
[2015-07-21 17:33:14,707] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 2 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,707] TRACE Controller 3 epoch 110 sending become-follower LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 1 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,708] TRACE Broker 2 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 14 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,708] TRACE Broker 2 handling LeaderAndIsr request correlationId 14 from controller 3 epoch 110 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,708] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 14 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,709] TRACE Broker 1 handling LeaderAndIsr request correlationId 14 from controller 3 epoch 110 starting the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,709] TRACE Broker 2 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 14 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,709] TRACE Broker 1 stopped fetchers as part of become-follower request from controller 3 epoch 110 with correlation id 14 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,711] TRACE Broker 2 truncated logs and checkpointed recovery boundaries for partition [t1,4] as part of become-follower request with correlation id 14 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,711] TRACE Broker 2 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 14 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,712] TRACE Broker 2 completed LeaderAndIsr request correlationId 14 from controller 3 epoch 110 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,712] TRACE Broker 1 truncated logs and checkpointed recovery boundaries for partition [t1,4] as part of become-follower request with correlation id 14 from controller 3 epoch 110 (state.change.logger)
[2015-07-21 17:33:14,712] TRACE Broker 1 started fetcher to new leader as part of become-follower request from controller 3 epoch 110 with correlation id 14 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,712] TRACE Broker 1 completed LeaderAndIsr request correlationId 14 from controller 3 epoch 110 for the become-follower transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,713] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(14,Map((t1,4) -> 0),0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,715] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(14,Map((t1,4) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:14,723] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 3 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,723] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 2 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,724] TRACE Broker 3 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) correlation id 14 from controller 3 epoch 110 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,724] TRACE Broker 3 handling LeaderAndIsr request correlationId 14 from controller 3 epoch 110 starting the become-leader transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,724] TRACE Broker 3 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 14 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,725] TRACE Broker 3 completed LeaderAndIsr request correlationId 14 from controller 3 epoch 110 for the become-leader transition for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,725] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 14 (state.change.logger)
[2015-07-21 17:33:14,726] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(14,Map((t1,4) -> 0),0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,727] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(14,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:14,728] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 1 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,728] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110) with correlationId 14 to broker 3 for partition [t1,4] (state.change.logger)
[2015-07-21 17:33:14,730] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 14 (state.change.logger)
[2015-07-21 17:33:14,731] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:3,ISR:2,1,3,LeaderEpoch:4,ControllerEpoch:110),ReplicationFactor:3),AllReplicas:3,1,2) for partition [t1,4] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 14 (state.change.logger)
[2015-07-21 17:33:14,732] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(14,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:14,732] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(14,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:27,187] TRACE Controller 3 epoch 110 changed partition [tT,0] state from NonExistentPartition to NewPartition with assigned replicas 1 (state.change.logger)
[2015-07-21 17:33:27,188] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [tT,0] from NonExistentReplica to NewReplica (state.change.logger)
[2015-07-21 17:33:27,196] TRACE Controller 3 epoch 110 changed partition [tT,0] from NewPartition to OnlinePartition with leader 1 (state.change.logger)
[2015-07-21 17:33:27,196] TRACE Controller 3 epoch 110 sending become-leader LeaderAndIsr request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110) with correlationId 17 to broker 1 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,196] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110) with correlationId 17 to broker 2 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,196] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110) with correlationId 17 to broker 1 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,196] TRACE Controller 3 epoch 110 sending UpdateMetadata request (Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110) with correlationId 17 to broker 3 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,197] TRACE Controller 3 epoch 110 changed state of replica 1 for partition [tT,0] from NewReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:33:27,197] TRACE Broker 1 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110),ReplicationFactor:1),AllReplicas:1) correlation id 17 from controller 3 epoch 110 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,198] TRACE Broker 3 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110),ReplicationFactor:1),AllReplicas:1) for partition [tT,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 17 (state.change.logger)
[2015-07-21 17:33:27,198] TRACE Broker 1 handling LeaderAndIsr request correlationId 17 from controller 3 epoch 110 starting the become-leader transition for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,198] TRACE Broker 2 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110),ReplicationFactor:1),AllReplicas:1) for partition [tT,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 17 (state.change.logger)
[2015-07-21 17:33:27,199] TRACE Broker 1 stopped fetchers as part of become-leader request from controller 3 epoch 110 with correlation id 17 for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,199] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(17,0) for a request sent to broker id:3,host:10.15.104.47,port:9094 (state.change.logger)
[2015-07-21 17:33:27,199] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(17,0) for a request sent to broker id:2,host:10.15.104.47,port:9093 (state.change.logger)
[2015-07-21 17:33:27,206] TRACE Broker 1 completed LeaderAndIsr request correlationId 17 from controller 3 epoch 110 for the become-leader transition for partition [tT,0] (state.change.logger)
[2015-07-21 17:33:27,206] TRACE Controller 3 epoch 110 received response LeaderAndIsrResponse(17,Map((tT,0) -> 0),0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:33:27,207] TRACE Broker 1 cached leader info (LeaderAndIsrInfo:(Leader:1,ISR:1,LeaderEpoch:0,ControllerEpoch:110),ReplicationFactor:1),AllReplicas:1) for partition [tT,0] in response to UpdateMetadata request sent by controller 3 epoch 110 with correlation id 17 (state.change.logger)
[2015-07-21 17:33:27,208] TRACE Controller 3 epoch 110 received response UpdateMetadataResponse(17,0) for a request sent to broker id:1,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:38:15,563] DEBUG preRegister called. Server=com.sun.jmx.mbeanserver.JmxMBeanServer@66d3c617, name=log4j:logger=state.change.logger (state.change.logger)
[2015-07-21 17:39:27,878] TRACE Controller 0 epoch 111 changed partition [t1,2] state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2015-07-21 17:39:27,887] TRACE Controller 0 epoch 111 changed partition [t1,0] state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2015-07-21 17:39:27,896] TRACE Controller 0 epoch 111 changed partition [t1,1] state from NonExistentPartition to NewPartition with assigned replicas 0 (state.change.logger)
[2015-07-21 17:39:27,918] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,2] from NonExistentReplica to NewReplica (state.change.logger)
[2015-07-21 17:39:27,920] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,0] from NonExistentReplica to NewReplica (state.change.logger)
[2015-07-21 17:39:27,921] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,1] from NonExistentReplica to NewReplica (state.change.logger)
[2015-07-21 17:39:27,956] TRACE Controller 0 epoch 111 changed partition [t1,2] from NewPartition to OnlinePartition with leader 0 (state.change.logger)
[2015-07-21 17:39:27,960] TRACE Controller 0 epoch 111 changed partition [t1,0] from NewPartition to OnlinePartition with leader 0 (state.change.logger)
[2015-07-21 17:39:27,966] TRACE Controller 0 epoch 111 changed partition [t1,1] from NewPartition to OnlinePartition with leader 0 (state.change.logger)
[2015-07-21 17:39:27,969] TRACE Controller 0 epoch 111 sending become-leader LeaderAndIsr request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:27,969] TRACE Controller 0 epoch 111 sending become-leader LeaderAndIsr request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:27,969] TRACE Controller 0 epoch 111 sending become-leader LeaderAndIsr request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:27,972] TRACE Controller 0 epoch 111 sending UpdateMetadata request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:27,972] TRACE Controller 0 epoch 111 sending UpdateMetadata request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:27,972] TRACE Controller 0 epoch 111 sending UpdateMetadata request (Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111) with correlationId 7 to broker 0 for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:27,976] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,2] from NewReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:39:27,976] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,0] from NewReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:39:27,977] TRACE Controller 0 epoch 111 changed state of replica 0 for partition [t1,1] from NewReplica to OnlineReplica (state.change.logger)
[2015-07-21 17:39:27,980] TRACE Broker 0 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) correlation id 7 from controller 0 epoch 111 for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:27,980] TRACE Broker 0 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) correlation id 7 from controller 0 epoch 111 for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:27,980] TRACE Broker 0 received LeaderAndIsr request (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) correlation id 7 from controller 0 epoch 111 for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:27,986] TRACE Broker 0 handling LeaderAndIsr request correlationId 7 from controller 0 epoch 111 starting the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:27,986] TRACE Broker 0 handling LeaderAndIsr request correlationId 7 from controller 0 epoch 111 starting the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:27,986] TRACE Broker 0 handling LeaderAndIsr request correlationId 7 from controller 0 epoch 111 starting the become-leader transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:27,988] TRACE Broker 0 stopped fetchers as part of become-leader request from controller 0 epoch 111 with correlation id 7 for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:27,988] TRACE Broker 0 stopped fetchers as part of become-leader request from controller 0 epoch 111 with correlation id 7 for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:27,988] TRACE Broker 0 stopped fetchers as part of become-leader request from controller 0 epoch 111 with correlation id 7 for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:28,040] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 111 for the become-leader transition for partition [t1,1] (state.change.logger)
[2015-07-21 17:39:28,040] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 111 for the become-leader transition for partition [t1,2] (state.change.logger)
[2015-07-21 17:39:28,040] TRACE Broker 0 completed LeaderAndIsr request correlationId 7 from controller 0 epoch 111 for the become-leader transition for partition [t1,0] (state.change.logger)
[2015-07-21 17:39:28,051] TRACE Controller 0 epoch 111 received response LeaderAndIsrResponse(7,Map((t1,2) -> 0, (t1,0) -> 0, (t1,1) -> 0),0) for a request sent to broker id:0,host:10.15.104.47,port:9092 (state.change.logger)
[2015-07-21 17:39:28,059] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) for partition [t1,2] in response to UpdateMetadata request sent by controller 0 epoch 111 with correlation id 7 (state.change.logger)
[2015-07-21 17:39:28,060] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) for partition [t1,0] in response to UpdateMetadata request sent by controller 0 epoch 111 with correlation id 7 (state.change.logger)
[2015-07-21 17:39:28,060] TRACE Broker 0 cached leader info (LeaderAndIsrInfo:(Leader:0,ISR:0,LeaderEpoch:0,ControllerEpoch:111),ReplicationFactor:1),AllReplicas:0) for partition [t1,1] in response to UpdateMetadata request sent by controller 0 epoch 111 with correlation id 7 (state.change.logger)
[2015-07-21 17:39:28,062] TRACE Controller 0 epoch 111 received response UpdateMetadataResponse(7,0) for a request sent to broker id:0,host:10.15.104.47,port:9092 (state.change.logger)
